{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Analysis to determine election sentiments in Hudson County"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly, we import the essential libraries\n",
    "* Tweepy is the Twitter client that helps with the retrieval and manipulation of Twitter posts\n",
    "* nltk and textblob are text processing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We obtain the authorization parameters from our Twitter developer account\n",
    "* Then we initiate the API object direclty to interact with Twitter using the tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key=\"...\"\n",
    "consumer_secret=\"...\"\n",
    "access_token=\"...\"\n",
    "access_token_secret=\"...\"\n",
    "\n",
    "auth=tw.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api=tw.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We put filter words \"Republicans\" and \"Democrats\" to find the sentiments associated with them\n",
    "* We further use cursor to iterate through the API and fetch the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words_1=\"Republicans\"\n",
    "search_words_2=\"Democrats\"\n",
    "new_search_1=search_words_1+\" -filter:retweets\"\n",
    "new_search_2=search_words_2+\" -filter:retweets\"\n",
    "\n",
    "date_since = \"2020-7-1\"\n",
    "\n",
    "tweets_1 = tw.Cursor(api.search,q=new_search_1,lang=\"en\",since=date_since,location=(74.0535,40.7453,62.31)).items(200)\n",
    "tweets_2 = tw.Cursor(api.search,q=new_search_2,lang=\"en\",since=date_since,location=(74.0535,40.7453,62.31)).items(200)\n",
    "\n",
    "user_locs_1 = [[tweet.user.screen_name, tweet.text] for tweet in tweets_1]\n",
    "user_locs_2 = [[tweet.user.screen_name, tweet.text] for tweet in tweets_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_1=pd.DataFrame(data=user_locs_1,columns=[\"user\",\"Tweets\"])\n",
    "tweet_text_2=pd.DataFrame(data=user_locs_2,columns=[\"user\",\"Tweets\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we consider the Republicans searchword. We start the process of Natural Language Processing and break the sentence in corresponding word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'SimplyMargolous', 'Stupid', '@', 'SenateGOP', 'you', 'could', 'have', 'impeached', 'him', 'and', 'Pence', 'could', 'have', 'won', '2nd', 'term', '.....', 'Republicansâ€¦', 'https', ':', '//t.co/HY8pJkGgD0']\n"
     ]
    }
   ],
   "source": [
    "trial = []\n",
    "for i in tweet_text_1[\"Tweets\"]:\n",
    "    trial.append(word_tokenize(i))\n",
    "\n",
    "word_tokens = word_tokenize(str(tweet_text_1[\"Tweets\"])) \n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words = list(stop_words)\n",
    "stop_words\n",
    "print(trial[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out the stop words here and append them to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = []\n",
    "\n",
    "for w in trial:\n",
    "    array1 = []\n",
    "    for j in w:\n",
    "        if j.lower() not in stop_words:\n",
    "            array1.append(j)\n",
    "    array.append(array1)\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "j=0\n",
    "for i in array:\n",
    "    a.append(\" \".join(array[j]))\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following part we make use of the Regular Expression library to remove noise from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' guess got warning beat  hands looks like mouth think    beqUQOhL'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "pattern=[]\n",
    "for j in a:\n",
    "    pattern.append(re.sub(r\"(\\@\\s\\w+)|(\\s\\.)|(\\'\\w+)|https?[A-Za-z0-9]+|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|\\d|\\b[a-zA-Z][a-zA-Z]\\b|\\b[a-zA-Z]\\b\",\"\",a[i]))\n",
    "    i=i+1\n",
    "pattern[128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_emotion(tweets):\n",
    "    analysis=TextBlob(tweets)\n",
    "    if analysis.sentiment.polarity>0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "z=np.array([analyse_emotion(tweet) for tweet in pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=np.array(pattern)\n",
    "z=pd.DataFrame(z)\n",
    "pattern=pd.DataFrame(pattern)\n",
    "\n",
    "pattern[\"polarity\"]=z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    80\n",
       " 1    79\n",
       "-1    41\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern[\"polarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment is positive!\n",
      "The positivity index out of 100 is : 19.0\n"
     ]
    }
   ],
   "source": [
    "sum1 = sum(pattern[\"polarity\"])\n",
    "scale_of_positivity = sum1/2\n",
    "\n",
    "if sum1>0:\n",
    "    print(\"Sentiment is positive!\")\n",
    "    print(\"The positivity index out of 100 is :\",scale_of_positivity)\n",
    "elif sum1<0:\n",
    "    print(\"Sentiment is negative!\")\n",
    "    print(\"The negativity index out of 100 is :\",scale_of_positivity)\n",
    "else:\n",
    "    print(\"Sentiment is neutral!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can run the code for Democrats as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment is positive!\n",
      "The positivity index out of 100 is : 14.5\n"
     ]
    }
   ],
   "source": [
    "trial2 = []\n",
    "for i in tweet_text_2[\"Tweets\"]:\n",
    "    trial2.append(word_tokenize(i))\n",
    "word_tokens2 = word_tokenize(str(tweet_text_2[\"Tweets\"])) \n",
    "\n",
    "\n",
    "\n",
    "array = []\n",
    "for w in trial2:\n",
    "    array1 = []\n",
    "    for j in w:\n",
    "        if j.lower() not in stop_words:\n",
    "            array1.append(j)\n",
    "    array.append(array1)\n",
    " \n",
    "\n",
    "a=[]\n",
    "j=0\n",
    "for i in array:\n",
    "    a.append(\" \".join(array[j]))\n",
    "    j=j+1\n",
    "   \n",
    "\n",
    "\n",
    "i=0\n",
    "pattern2=[]\n",
    "for j in a:\n",
    "    pattern2.append(re.sub(r\"(\\@\\s\\w+)|(\\s\\.)|(\\'\\w+)|https?[A-Za-z0-9]+|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|\\d|\\b[a-zA-Z][a-zA-Z]\\b|\\b[a-zA-Z]\\b\",\"\",a[i]))\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "z=np.array([analyse_emotion(tweet) for tweet in pattern2])\n",
    "\n",
    "\n",
    "\n",
    "pattern2=np.array(pattern2)\n",
    "z=pd.DataFrame(z)\n",
    "pattern2=pd.DataFrame(pattern2)\n",
    "pattern2[\"polarity\"]=z\n",
    "\n",
    "\n",
    "\n",
    "sum2 = sum(pattern2[\"polarity\"])\n",
    "scale_of_positivity = sum2/2\n",
    "\n",
    "if sum1>0:\n",
    "    print(\"Sentiment is positive!\")\n",
    "    print(\"The positivity index out of 100 is :\",scale_of_positivity)\n",
    "elif sum1<0:\n",
    "    print(\"Sentiment is negative!\")\n",
    "    print(\"The negativity index out of 100 is :\",scale_of_positivity)\n",
    "else:\n",
    "    print(\"Sentiment is neutral!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We conclude that the overall sentiment is positive for both. But the positivity index for Republicans is slightly higher than that of Democrats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
